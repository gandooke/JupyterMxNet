{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Configuring Jupyter On Amazon Deep Learning AMI\n",
    "\n",
    "Using this tutorial you will be able to install and run Jupyter using https on a Amazon Deep Learning AMI. AWS now also offers a Ubuntu version of the Deep Learning AMI, most of the following steps are compatible with both versions of the Deep Learning AMI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Importing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import images into Jupyter by using the following Python code, this is helpful in order to make your notebooks easy to understand or to link diagrams of your neural networks. Here we pull a file from S3 into the notebook and then display it with a Ipython.display object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url=\"https://s3-us-west-2.amazonaws.com/mxnetjupytersetup/Untitled.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now that you have launched the AWS Deep Learning AMI, and configured Jupyter you can run the next tutorials for learning how to use Jupyter, and many other MXNet tutorials. Check out the MXNet GitHub Repo ( https://github.com/dmlc/mxnet-notebooks) for tutorials that make it easy to start using Jupyter for computer vision, natural language processing, and recommendation systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Importing MXNet Notebooks\n",
    "The following steps teach you how to pull notebooks from GitHub and how to sync your newly create Jupyter notebooks to S3 for backup. You can run these AWS CLI commands from your instance command prompt in order to store and sync your notebook files to S3. In Jupyter we can use the %bash operator to let Jupyter know that we want the commands that follow to be executed using the bash shell on the filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/ec2-user/notebook\n",
    "git clone https://github.com/dmlc/mxnet-notebooks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "The command above pulls the CNN MXNet tutorial from the AWS MXNet github repo and places it into Jupyter as a notebook you can use to train a CNN. You can now load this notebook from the Jupyter console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url=\"https://s3-us-west-2.amazonaws.com/mxnetjupytersetup/MxNetNotebook.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "In order to use the sample notebooks you need to make sure all the notebook dependencies are installed in your enviroment, the steps for doing so are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo pip install -U scikit-learn\n",
    "cd /home/ec2-user/src/anaconda3/bin\n",
    "sudo ./conda install graphviz -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "After running the above command, wait till for the Kernel output before continuing. You can check if the Kernel is still busy executing a command at the top right corner of the screen. MXNet comes pre-installed on the AWS Deep Learning AMI, now by going into the mxnet-notebooks folder you can run all sort of deep learning tutorials, from handwritting reccognition to a LSTM neural network. Check them out and have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Backing Up Notebooks to S3\n",
    "Backing your notebooks up to S3 is a good way to ensure that you do not lose all the hard work you have put into creating them. The steps for doing so are below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Create a S3 bucket in the AWS console, then use the following commands to backup your notebooks to S3, remmber you need to update the bucket name jupyternotebooks to your own S3 bucket in the script below in order for it to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp /home/ec2-user/notebook/ s3://jupyternotebooks --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Again wait for the Kernel to finish the command and give you an output. If successful you will now have a backup of all of your notebooks in S3 that you can pull or push to from the AWS CLI on your AWS Deep Learning AMI instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling AWS APIs From Jupyter \n",
    "\n",
    "Calling AWS APIs from Jupyter uses the same techniques we have already shown, both the %%bash command and then the AWS CLI commands. Below find example code that runs Python code thorugh AWS Polly for text to speech translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install the AWS SDK for Python (boto3) in our Anaconda installation, the steps for doing so are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/ec2-user/src/anaconda3/bin\n",
    "sudo ./conda install boto3 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the Kernel to finish the command and give you an output. If successful boto3 will be installed into your Jupyter enviroment. Now we can interact with AWS API's through the Jupyter notebook. As a sample, let try and call AWS Polly to convert some text to speech from our notebook. The code to do just that is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Getting Started Example for Python 2.7+/3.3+\"\"\"\n",
    "from boto3 import Session\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "from contextlib import closing\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from tempfile import gettempdir\n",
    "\n",
    "# Create a client using the credentials and region defined in the [default]\n",
    "# section of the AWS credentials file (~/.aws/credentials).\n",
    "session = Session(profile_name=\"default\")\n",
    "polly = session.client(\"polly\")\n",
    "\n",
    "try:\n",
    "    # Request speech synthesis\n",
    "    response = polly.synthesize_speech(Text=\"Hello and welcome to the configuring Jupyter on Amazon Deep Learning AMI tutorial!\", OutputFormat=\"mp3\",\n",
    "                                        VoiceId=\"Joanna\")\n",
    "except (BotoCoreError, ClientError) as error:\n",
    "    # The service returned an error, exit gracefully\n",
    "    print(error)\n",
    "    sys.exit(-1)\n",
    "\n",
    "# Access the audio stream from the response\n",
    "if \"AudioStream\" in response:\n",
    "    # Note: Closing the stream is important as the service throttles on the\n",
    "    # number of parallel connections. Here we are using contextlib.closing to\n",
    "    # ensure the close method of the stream object will be called automatically\n",
    "    # at the end of the with statement's scope.\n",
    "    with closing(response[\"AudioStream\"]) as stream:\n",
    "        output = os.path.join(gettempdir(), \"speech.mp3\")\n",
    "\n",
    "        try:\n",
    "            # Open a file for writing the output as a binary stream\n",
    "            with open(output, \"wb\") as file:\n",
    "                file.write(stream.read())\n",
    "        except IOError as error:\n",
    "            # Could not write to file, exit gracefully\n",
    "            print(error)\n",
    "            sys.exit(-1)\n",
    "\n",
    "else:\n",
    "    # The response didn't contain audio data, exit gracefully\n",
    "    print(\"Could not stream audio\")\n",
    "    sys.exit(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above calls AWS Polly and then converts text to speech and stores a speech.mp3 file on the /tmp drive of the filesystem. Lets make sure that happened, by checking the filesystem for the speech.mp3 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /tmp\n",
    "ls -ltr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the speech.mp3 file is now on the filesystem. Lets back it up to S3 as we did with the notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp --acl public-read /tmp/speech.mp3 s3://pollysamplebucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go into our bucket pollysamplebucket we can now see that the speech.mp3 output for Polly is now stored in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Embedding Files in a Notebook\n",
    "Throughout the course of this tutorial we have added images into the notebook through python code. There are many ways to add files into a notebook. Check out the python code above the images in this post and try and post your own image inside of a notebook using the format. Below we will discuss serveral other file types you can add to your notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Below we import the speech.mp3 auto file that we just converted from text to speech by AWS Polly into our Jupyter notebook and allow the user of the notebook to play them with an Ipython.display.Audio object. This is a simple use case but it can be very handy later on for deep learning tasks that require audio, for example, natural language processing engines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.display(IPython.display.Audio(url=\"\thttps://s3-us-west-2.amazonaws.com/pollysamplebucket/speech.mp3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "After going through this tutorial you should be able to stand up a fully functional Jupyter notebook on the AWS Deep Learning AMI and use MXNet to run and create your own deep learning architectures. A few sample excercises to test your knowlage are as follows:\n",
    "* Deploy an AWS Deep Learning instance\n",
    "* Setup Jupyter on your own AWS Deep Learning instance \n",
    "* Create your first Jupyter notebook\n",
    "* Call AWS services through the Jupyter notebook\n",
    "* Import an image from S3 into your notebook\n",
    "* Import an audio file from S3 into your notebook\n",
    "\n",
    "Work Hard, Have Fun, and, Make History!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
